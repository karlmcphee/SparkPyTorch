{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (20.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (1.15.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (1.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (0.19.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python2.7/dist-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python2.7/dist-packages (0.9.0)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python2.7/dist-packages (1.5.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python2.7/dist-packages (from matplotlib) (1.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: subprocess32 in /usr/local/lib/python2.7/dist-packages (from matplotlib) (3.5.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python2.7/dist-packages (from wordcloud) (5.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy pandas scikit-learn matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.1.0-cp27-cp27mu-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |################################| 421.8 MB 6.4 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.2.2; python_version < \"3\"\n",
      "  Downloading scipy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |################################| 24.8 MB 53.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: functools32>=3.2.3; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.2.3.post2)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.0-py2-none-any.whl (3.8 MB)\n",
      "\u001b[K     |################################| 3.8 MB 55.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |################################| 42 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.31.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-2.3.2.tar.gz (59 kB)\n",
      "\u001b[K     |################################| 59 kB 9.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.16.0\n",
      "  Downloading numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |################################| 17.0 MB 50.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.12.0.tar.gz (112 kB)\n",
      "\u001b[K     |################################| 112 kB 76.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8.tar.gz (289 kB)\n",
      "\u001b[K     |################################| 289 kB 74.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.37.1-cp27-cp27mu-manylinux2010_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |################################| 4.0 MB 59.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |################################| 448 kB 80.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.6)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py2-none-any.whl (57 kB)\n",
      "\u001b[K     |################################| 57 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0; python_version < \"3\"\n",
      "  Downloading mock-3.0.5-py2.py3-none-any.whl (25 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.15.8-cp27-cp27mu-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |################################| 1.0 MB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting backports.weakref>=1.0rc1; python_version < \"3.4\"\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-44.1.1-py2.py3-none-any.whl (583 kB)\n",
      "\u001b[K     |################################| 583 kB 72.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.0)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |################################| 61 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.1.1-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |################################| 87 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |################################| 298 kB 74.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.0-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |################################| 146 kB 75.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |################################| 2.8 MB 58.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting funcsigs>=1; python_version < \"3.3\"\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2018.4.16)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.23)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.7)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<4.6; python_version < \"3.6\"\n",
      "  Downloading rsa-4.5-py2.py3-none-any.whl (36 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |################################| 155 kB 78.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |################################| 147 kB 71.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |################################| 77 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: opt-einsum, termcolor, absl-py, keras-applications, wrapt, gast\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-py2-none-any.whl size=49064 sha256=ecb802fc325bcc7996ff5155387fa3072f7231947189eefb31cf6f2ec51bcc99\n",
      "  Stored in directory: /root/.cache/pip/wheels/ef/c4/c2/d0b07dd2a54f4d583a5de0e6ce5eb7a1832961b9a10d1ec953\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py2-none-any.whl size=4003 sha256=49c3184b08dac1289cec0bbec3dc19cb9c90f0755772ec51158fd396319f60a9\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/54/87/2f4d1a48c87e43906477a3c93d9663c49ca092046d5a4b00b4\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.12.0-py2-none-any.whl size=124865 sha256=2d6d1303932c148dc35c4e2d994370147e029c3dc13ceb1d129075ce5190d0f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/e2/c5/527a67e1c2c38d716d3faefb8bd238d7ede973c55e2bb22f80\n",
      "  Building wheel for keras-applications (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-applications: filename=Keras_Applications-1.0.8-py2-none-any.whl size=49635 sha256=a3673f0a4233411db2544d21e2ad869ca2d576b81fc1f45fd30012d957c54177\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/a0/64/e2e0c93740e0460f4b7f036141b7c73b5e44ff38f690ddff9f\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp27-cp27mu-linux_x86_64.whl size=61057 sha256=8c48e43bcfc0dd82df00fb60b85ed2e788e32fc00584c0b68984118347556238\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/d8/8e/81a83cb5321b940a954996f5b57fddc8976e712b3ac3a1a54b\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py2-none-any.whl size=6587 sha256=9bd34a80ac3c8920300589e4823d9236d82a6a162ceecf4b355d1c976f0ad39f\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/10/f7/29260ef8a721b90061c8c70a4f0130a64036e8dafe15acc097\n",
      "Successfully built opt-einsum termcolor absl-py keras-applications wrapt gast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: numpy, scipy, six, setuptools, grpcio, requests, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, protobuf, markdown, werkzeug, absl-py, tensorboard, keras-preprocessing, opt-einsum, termcolor, h5py, keras-applications, wrapt, tensorflow-estimator, gast, google-pasta, funcsigs, mock, astor, backports.weakref, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.15.0\n",
      "    Uninstalling numpy-1.15.0:\n",
      "      Successfully uninstalled numpy-1.15.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.1.0\n",
      "    Uninstalling scipy-1.1.0:\n",
      "      Successfully uninstalled scipy-1.1.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.0.0\n",
      "    Uninstalling setuptools-40.0.0:\n",
      "      Successfully uninstalled setuptools-40.0.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.19.1\n",
      "    Uninstalling requests-2.19.1:\n",
      "      Successfully uninstalled requests-2.19.1\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 backports.weakref-1.0.post1 cachetools-3.1.1 funcsigs-1.0.2 gast-0.2.2 google-auth-1.30.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.37.1 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.1.1 mock-3.0.5 numpy-1.16.6 oauthlib-3.1.0 opt-einsum-2.3.2 protobuf-3.15.8 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.5 scipy-1.2.2 setuptools-44.1.1 six-1.15.0 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python2.7/dist-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.16.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (44.1.1)\n",
      "Requirement already satisfied: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard) (3.2.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.37.1)\n",
      "Requirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (3.15.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: enum34>=1.0.4; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.24.3->tensorboard) (1.1.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2018.4.16)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.23)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.6; python_version < \"3.6\" in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa<4.6; python_version < \"3.6\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /tmp/bigdl_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import random as rd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bigdl.dataset import news20\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.util.common import Sample\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.dataset import mnist\n",
    "from bigdl.util.common import *\n",
    "from bigdl.util.common import Sample\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_dim = 50\n",
    "sequence_len = 50\n",
    "max_words = 1000 \n",
    "training_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(sc, data_type=\"train\", location=\"/tmp/mnist\"):\n",
    "    \"\"\"\n",
    "    Get and normalize the mnist data. We would download it automatically\n",
    "    if the data doesn't present at the specific location.\n",
    "\n",
    "    :param sc: SparkContext\n",
    "    :param data_type: training data or testing data\n",
    "    :param location: Location storing the mnist\n",
    "    :return: A RDD of (features: Ndarray, label: Ndarray)\n",
    "    \"\"\"\n",
    "    (images, labels) = mnist.read_data_sets(location, data_type)\n",
    "    images = sc.parallelize(images)\n",
    "    labels = sc.parallelize(labels + 1) # Target start from 1 in BigDL\n",
    "    record = images.zip(labels)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.dataset import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', 'train-images-idx3-ubyte.gz')\n",
      "('Extracting', 'train-labels-idx1-ubyte.gz')\n",
      "('Extracting', 't10k-images-idx3-ubyte.gz')\n",
      "('Extracting', 't10k-labels-idx1-ubyte.gz')\n"
     ]
    }
   ],
   "source": [
    "train_data = get_mnist(sc, \"train\", \"\")\\\n",
    "    .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n",
    "                       rec_tuple[1]))\\\n",
    "    .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n",
    "test_data = get_mnist(sc, \"test\", \"\")\\\n",
    "    .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TEST_MEAN, mnist.TEST_STD),\n",
    "                       rec_tuple[1]))\\\n",
    "    .map(lambda t: Sample.from_ndarray(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createReshape\n",
      "creating: createSpatialConvolution\n",
      "creating: createTanh\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createTanh\n",
      "creating: createSpatialConvolution\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createReshape\n",
      "creating: createLinear\n",
      "creating: createTanh\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.nn.layer.Sequential at 0x7f3d16142810>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape([1, 28, 28]))\n",
    "model.add(SpatialConvolution(1, 6, 5, 5))\n",
    "model.add(Tanh())\n",
    "model.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model.add(Tanh())\n",
    "model.add(SpatialConvolution(6, 12, 5, 5))\n",
    "model.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model.add(Reshape([12 * 4 * 4]))\n",
    "model.add(Linear(12 * 4 * 4, 100))\n",
    "model.add(Tanh())\n",
    "model.add(Linear(100, 100))\n",
    "model.add(LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-4fce39ebb4bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer = Optimizer(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtraining_rdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mClassNLLCriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mend_trigger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMaxEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = Optimizer(\n",
    "    model=model,\n",
    "    training_rdd=train_data,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    end_trigger=MaxEpoch(3),\n",
    "    batch_size=batch_size,\n",
    "    optim_method=Adagrad(learningrate=.002, learningrate_decay=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "    optimizer.set_validation(\n",
    "        batch_size=batch_size,\n",
    "        val_rdd=test_data,\n",
    "        trigger=EveryEpoch(),\n",
    "        val_method=[Top1Accuracy()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.optim.optimizer.Optimizer at 0x7fd85ab84350>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    logdir='/tmp/bigdl_summaries'\n",
    "    train_summary = TrainSummary(log_dir=logdir, app_name=\"app_name\")\n",
    "    train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "    val_summary = ValidationSummary(log_dir=logdir, app_name=\"app_name\")\n",
    "    optimizer.set_train_summary(train_summary)\n",
    "    optimizer.set_val_summary(val_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.util.common import *\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "from bigdl.dataset import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is 37.7809531689\n"
     ]
    }
   ],
   "source": [
    "startt = time.time()\n",
    "trained_model = optimizer.optimize()\n",
    "endt = time.time()\n",
    "print(\"Time taken is \" + str(endt-startt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTop1Accuracy\n",
      "Evaluated result: 0.883899986744, total_num: 10000, method: Top1Accuracy\n"
     ]
    }
   ],
   "source": [
    "results = trained_model.evaluate(test_data, 128, [Top1Accuracy()])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-0.2.2.tar.gz (47 kB)\n",
      "\u001b[K     |################################| 47 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting boto3>=1.9.91\n",
      "  Downloading boto3-1.17.61-py2.py3-none-any.whl (131 kB)\n",
      "\u001b[K     |################################| 131 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore>=1.12.91\n",
      "  Downloading botocore-1.20.61-py2.py3-none-any.whl (7.5 MB)\n",
      "\u001b[K     |################################| 7.5 MB 39.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from s3fs) (1.15.0)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.5.0,>=0.4.0\n",
      "  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\n",
      "\u001b[K     |################################| 79 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "\u001b[K     |################################| 153 kB 80.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python2.7/dist-packages (from botocore>=1.12.91->s3fs) (2.7.3)\n",
      "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from s3transfer<0.5.0,>=0.4.0->boto3>=1.9.91->s3fs) (3.2.0)\n",
      "Building wheels for collected packages: s3fs\n",
      "  Building wheel for s3fs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for s3fs: filename=s3fs-0.2.2-py2.py3-none-any.whl size=19738 sha256=50c828a735b775677e5f04c47f92356e7b5ebb57491be9c669225d3d2f1c6de0\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/fe/f2/0a4ecc11da0b15c90aeb5592b1b5f52f9f6c743e3e0d977deb\n",
      "Successfully built s3fs\n",
      "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3, s3fs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.23\n",
      "    Uninstalling urllib3-1.23:\n",
      "      Successfully uninstalled urllib3-1.23\n",
      "Successfully installed boto3-1.17.61 botocore-1.20.61 jmespath-0.10.0 s3fs-0.2.2 s3transfer-0.4.2 urllib3-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs.core import S3FileSystem\n",
    "s3 = S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "key2 = 't_data.npy'\n",
    "bucket2 = 'mybucket5981'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "key = 't_train.npy'\n",
    "bucket = 'mybucket5981'\n",
    "\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load(s3.open('{}/{}'.format(bucket2, key2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "key4 = 't_target.npy'\n",
    "df2 = np.load(s3.open('{}/{}'.format(bucket2, key4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "key5 = 'test_target.npy'\n",
    "df4 = np.load(s3.open('{}/{}'.format(bucket2, key5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4[0:50000]\n",
    "df2 = df2[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "x_train=[]\n",
    "for i in df:\n",
    "    i = i.astype('float32') / 255.\n",
    "    i2=i.reshape(-1,28)\n",
    "    x_train.append(i2)\n",
    "print(len(x_train[0]))\n",
    "x_train=x_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "for i in df2:\n",
    "    y_train.append(i)\n",
    "y_train=y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "key3 = 'test_data.npy'\n",
    "df3 = np.load(s3.open('{}/{}'.format(bucket2, key3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "key4 = 'test_target.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = np.load(s3.open('{}/{}'.format(bucket2, key4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "x_test=[]\n",
    "df3 = df3[0:50000]\n",
    "for i in df3:\n",
    "    i = i.astype('float32') / 255.\n",
    "    i2=i.reshape(-1,28)\n",
    "    x_test.append(i2)\n",
    "print(len(x_test[0]))\n",
    "x_test=x_test[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train5=[]\n",
    "for i in x_train:\n",
    "    i = np.expand_dims(i, axis=2)\n",
    "    x_train5.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test5 = []\n",
    "for i in x_test:\n",
    "    i = np.expand_dims(i, axis=2)\n",
    "    x_test5.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2=[]\n",
    "df4 = df4[0:50000]\n",
    "for i in df4:\n",
    "    y_test2.append(i)\n",
    "y_test2=y_test2[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train5))\n",
    "print(len(y_train))\n",
    "print(len(y_test2))\n",
    "print(len(x_test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = np.array(y_train)\n",
    "y_train2 = y_train2+1\n",
    "y_test2 = np.array(y_test2)\n",
    "y_test2 = y_test2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train5=x_train5[20000:30000]\n",
    "x_test5=x_test5[20000:30000]\n",
    "y_train2=y_train2[20000:30000]\n",
    "y_test2=y_test2[20000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "    imagesr = sc.parallelize(x_train5)\n",
    "    labelsr = sc.parallelize(y_train2) # Target start from 1 in BigDL\n",
    "    recordr = imagesr.zip(labelsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "    imagesr2 = sc.parallelize(x_test5)\n",
    "    labelsr2 = sc.parallelize(y_test2) # Target start from 1 in BigDL\n",
    "    recordr2 = imagesr2.zip(labelsr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainw = recordr.map(lambda t: Sample.from_ndarray(t[0], t[1]))\n",
    "testw = recordr2.map(lambda t: Sample.from_ndarray(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createClassNLLCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdagrad\n",
      "creating: createDistriOptimizer\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = Optimizer(\n",
    "    model=model2,\n",
    "    training_rdd=trainw,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    end_trigger=MaxEpoch(20),\n",
    "    batch_size=batch_size,\n",
    "    optim_method=Adagrad(learningrate=.001, learningrate_decay=0.000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "    optimizer2.set_validation(\n",
    "        batch_size=batch_size,\n",
    "        val_rdd=testw,\n",
    "        trigger=EveryEpoch(),\n",
    "        val_method=[Top1Accuracy()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.optim.optimizer.Optimizer at 0x7f3d9a2118d0>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    logdir='/tmp/bigdl_summaries'\n",
    "    train_summary = TrainSummary(log_dir=logdir, app_name=\"app_name\")\n",
    "    train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "    val_summary = ValidationSummary(log_dir=logdir, app_name=\"app_name\")\n",
    "    optimizer2.set_train_summary(train_summary)\n",
    "    optimizer2.set_val_summary(val_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is 645.320491791\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startt = time.time()\n",
    "trained_model = optimizer2.optimize()\n",
    "endt = time.time()\n",
    "print(\"Time taken is \" + str(endt-startt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTop1Accuracy\n",
      "Evaluated result: 0.556500017643, total_num: 10000, method: Top1Accuracy\n"
     ]
    }
   ],
   "source": [
    "results = trained_model.evaluate(testw, 128, [Top1Accuracy()])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createReshape\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createReshape\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.nn.layer.Sequential at 0x7f3d9a211610>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Reshape([1, 28, 28]))\n",
    "model2.add(SpatialConvolution(1, 64, 3, 3))\n",
    "model2.add(ReLU())\n",
    "model2.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model2.add(SpatialConvolution(64, 256, 3, 3))\n",
    "model2.add(ReLU())\n",
    "model2.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model2.add(SpatialConvolution(256, 512, 3, 3))\n",
    "model2.add(ReLU())\n",
    "model2.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model2.add(Reshape([512]))\n",
    "model2.add(Linear(512, 100))\n",
    "model2.add(LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createReshape\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createReshape\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.nn.layer.Sequential at 0x7f3d9a214550>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Reshape([1, 28, 28]))\n",
    "model7.add(SpatialConvolution(1, 4, 3, 3))\n",
    "model7.add(ReLU())\n",
    "model7.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model7.add(SpatialConvolution(4, 8, 3, 3))\n",
    "model7.add(ReLU())\n",
    "model7.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model7.add(SpatialConvolution(8, 32, 3, 3))\n",
    "model7.add(ReLU())\n",
    "model7.add(SpatialMaxPooling(2, 2, 2, 2))\n",
    "model7.add(Reshape([128*3*3]))\n",
    "model7.add(Linear(1024, 100))\n",
    "model7.add(LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*3*3, 512),\n",
    "            nn.Linear(512, numclasses)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
