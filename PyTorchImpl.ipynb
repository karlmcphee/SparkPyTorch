{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python38\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch) (3.7.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\python38\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\python38\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: torch==1.8.1 in c:\\python38\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch==1.8.1->torchvision) (3.7.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, numclasses):\n",
    "        \"\"\"\n",
    "        args\n",
    "        \"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*3*3, 512),\n",
    "            nn.Linear(512, numclasses)\n",
    "        )\n",
    "\n",
    "    def forward(self, net):\n",
    "        net = self.layer1(net)\n",
    "        net = self.layer2(net)\n",
    "        net = self.layer3(net)\n",
    "        net = net.view(net.size(0), -1)\n",
    "        net = self.fc(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('C:/data/train/t_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.load('C:/data/train/t_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test= np.load('C:/data/test/test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= np.load('C:/data/test/test_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test[0:50000]\n",
    "y_test=y_test[0:50000]\n",
    "y_train=y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.11764706 0.2901961  0.3647059  0.39607844 0.23529412\n",
      "  0.03137255 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.03529412 0.35686275\n",
      "  0.7411765  0.99607843 1.         1.         1.         1.\n",
      "  0.9607843  0.17254902 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.00784314 0.50980395 0.9372549  1.\n",
      "  0.8392157  0.4509804  0.1882353  0.11372549 0.08627451 0.5411765\n",
      "  0.99607843 0.76862746 0.10588235 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01176471 0.6313726  1.         0.63529414 0.22745098\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30980393 0.9372549  0.9137255  0.16078432 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5803922  0.99607843 0.44705883 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.13333334 0.90588236 0.7411765  0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.28235295 0.99607843 0.50980395 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.38039216 1.         0.32156864 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.7764706  0.827451   0.01960784 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.00784314 0.7921569  0.85882354 0.00392157\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.16862746\n",
      "  1.         0.41568628 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.3529412  1.         0.16078432\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.5568628\n",
      "  0.9647059  0.06666667 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.10980392 1.         0.4\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.02352941 0.91764706\n",
      "  0.6392157  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.88235295 0.6156863\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.32941177 1.\n",
      "  0.24705882 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.75686276 0.7254902\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.54509807 0.9372549\n",
      "  0.00392157 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.6862745  0.7921569\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5921569  0.88235295\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.7490196  0.7294118\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.63529414 0.8392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.81960785 0.65882355\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.6784314  0.79607844\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.89411765 0.58431375\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.6901961  0.79607844\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.9647059  0.5137255\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5137255  0.972549\n",
      "  0.02745098 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.03921569 1.         0.43529412\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.29803923 1.\n",
      "  0.20784314 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.14117648 1.         0.35686275\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.08235294 0.99607843\n",
      "  0.44313726 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.46666667 0.99215686 0.10980392\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.7647059\n",
      "  0.8509804  0.01176471 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.8352941  0.7372549  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.3019608\n",
      "  1.         0.44313726 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.40392157 1.         0.36078432 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.00392157\n",
      "  0.64705884 0.9882353  0.3764706  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2627451  0.98039216 0.6784314  0.00784314 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.02745098 0.7019608  1.         0.6313726  0.11764706 0.\n",
      "  0.         0.         0.         0.         0.         0.03137255\n",
      "  0.43137255 0.9411765  0.8        0.03921569 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.00784314 0.48235294 0.95686275 0.972549   0.5882353\n",
      "  0.24313726 0.06666667 0.1254902  0.18431373 0.43137255 0.8980392\n",
      "  1.         0.7137255  0.09411765 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09019608 0.5686275  0.9529412\n",
      "  1.         1.         1.         1.         1.         0.7137255\n",
      "  0.21568628 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03529412\n",
      "  0.29411766 0.40392157 0.34901962 0.2901961  0.16470589 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "x_train2=[]\n",
    "for i in x_train:\n",
    "    i = i.astype('float32') / 255.\n",
    "    i2=i.reshape(-1,28)\n",
    "    x_train2.append(i2)\n",
    "print(len(x_train2[0]))\n",
    "print(x_train2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2=[]\n",
    "for i in y_train:\n",
    "    y_train2.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "x_test2=[]\n",
    "for i in x_test:\n",
    "    i = i.astype('float32') / 255.\n",
    "    i2=i.reshape(-1,28)\n",
    "    x_test2.append(i2)\n",
    "print(len(x_test2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3=[]\n",
    "for i in x_train2:\n",
    "    i = np.expand_dims(i, axis=0)\n",
    "    x_train3.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_test3[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test3=[]\n",
    "for i in x_test2:\n",
    "    i = np.expand_dims(i, axis=0)\n",
    "    x_test3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93. 44.  2. ... 95. 62. 31.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = []\n",
    "y_test2 = []\n",
    "for i in range(0,50000):\n",
    "    y_train2.append( torch.tensor(y_train[i]))\n",
    "    y_test2.append(torch.tensor(y_test[i]))\n",
    "for i in range(0, 50000):\n",
    "    y_train2[i] = y_train2[i].type(torch.LongTensor)\n",
    "    y_test2[i] = y_test2[i].type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65 50 93 ... 52 54 57]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train3))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "print(len(x_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_dataset=[]\n",
    "test_dataset=[]\n",
    "for i in range(30000, 40000):\n",
    "    train_dataset.append((x_train3[i], y_train2[i]))\n",
    "    test_dataset.append((x_test3[i], y_test2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-890c8d5dcf84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Track the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "seconds1 = time.time()\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "num_epochs=40\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if(i==0):\n",
    "            print(labels[i])\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "    endt = time.time()\n",
    "    print(\"Time is \" + str(endt-seconds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.11764706 0.2901961  0.3647059  0.39607844 0.23529412\n",
      "   0.03137255 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.03529412 0.35686275\n",
      "   0.7411765  0.99607843 1.         1.         1.         1.\n",
      "   0.9607843  0.17254902 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.00784314 0.50980395 0.9372549  1.\n",
      "   0.8392157  0.4509804  0.1882353  0.11372549 0.08627451 0.5411765\n",
      "   0.99607843 0.76862746 0.10588235 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.01176471 0.6313726  1.         0.63529414 0.22745098\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.30980393 0.9372549  0.9137255  0.16078432 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.5803922  0.99607843 0.44705883 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.13333334 0.90588236 0.7411765  0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.28235295 0.99607843 0.50980395 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.38039216 1.         0.32156864 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.7764706  0.827451   0.01960784 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.00784314 0.7921569  0.85882354 0.00392157\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.16862746\n",
      "   1.         0.41568628 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.3529412  1.         0.16078432\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.5568628\n",
      "   0.9647059  0.06666667 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.10980392 1.         0.4\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.02352941 0.91764706\n",
      "   0.6392157  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.88235295 0.6156863\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.32941177 1.\n",
      "   0.24705882 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.75686276 0.7254902\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.54509807 0.9372549\n",
      "   0.00392157 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.6862745  0.7921569\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.5921569  0.88235295\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.7490196  0.7294118\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.63529414 0.8392157\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.81960785 0.65882355\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.6784314  0.79607844\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.89411765 0.58431375\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.6901961  0.79607844\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.9647059  0.5137255\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.5137255  0.972549\n",
      "   0.02745098 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.03921569 1.         0.43529412\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.29803923 1.\n",
      "   0.20784314 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.14117648 1.         0.35686275\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.08235294 0.99607843\n",
      "   0.44313726 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.46666667 0.99215686 0.10980392\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.7647059\n",
      "   0.8509804  0.01176471 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.8352941  0.7372549  0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.3019608\n",
      "   1.         0.44313726 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.40392157 1.         0.36078432 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.00392157\n",
      "   0.64705884 0.9882353  0.3764706  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.2627451  0.98039216 0.6784314  0.00784314 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.02745098 0.7019608  1.         0.6313726  0.11764706 0.\n",
      "   0.         0.         0.         0.         0.         0.03137255\n",
      "   0.43137255 0.9411765  0.8        0.03921569 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.00784314 0.48235294 0.95686275 0.972549   0.5882353\n",
      "   0.24313726 0.06666667 0.1254902  0.18431373 0.43137255 0.8980392\n",
      "   1.         0.7137255  0.09411765 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.09019608 0.5686275  0.9529412\n",
      "   1.         1.         1.         1.         1.         0.7137255\n",
      "   0.21568628 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.03529412\n",
      "   0.29411766 0.40392157 0.34901962 0.2901961  0.16470589 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.05490196 0.3019608  0.04313726 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.65882355 1.         0.88235295 0.30588236\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.67058825 0.972549   0.80784315 1.\n",
      "   0.5568628  0.01176471 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.09803922\n",
      "   0.8745098  0.59607846 0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.28235295 1.         0.3764706  0.5254902\n",
      "   1.         0.6901961  0.04313726 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.6745098\n",
      "   1.         0.94509804 0.         0.        ]\n",
      "  [0.         0.         0.34117648 0.654902   0.76862746 0.8\n",
      "   0.8        0.8        0.8        0.94509804 0.9254902  0.56078434\n",
      "   0.6627451  0.9882353  0.7647059  0.16470589 0.13333334 0.13333334\n",
      "   0.13333334 0.07450981 0.06666667 0.06666667 0.21176471 1.\n",
      "   0.84705883 1.         0.05098039 0.        ]\n",
      "  [0.         0.13725491 0.972549   0.87058824 0.7058824  0.6666667\n",
      "   0.6666667  0.6666667  0.6666667  0.7647059  0.87058824 0.9254902\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         0.9529412\n",
      "   0.4        1.         0.15686275 0.        ]\n",
      "  [0.         0.3019608  0.99607843 0.76862746 0.11372549 0.\n",
      "   0.         0.         0.         0.         0.15294118 0.7529412\n",
      "   0.16470589 0.12156863 0.22745098 0.3137255  0.33333334 0.33333334\n",
      "   0.3882353  0.4        0.4        0.4        0.41960785 0.16862746\n",
      "   0.21960784 1.         0.2627451  0.        ]\n",
      "  [0.         0.         0.31764707 0.93333334 0.94509804 0.6666667\n",
      "   0.4117647  0.30588236 0.23921569 0.1764706  0.19607843 0.9098039\n",
      "   0.85882354 0.08235294 0.         0.         0.03529412 0.6509804\n",
      "   0.16862746 0.         0.         0.         0.         0.00784314\n",
      "   0.3019608  1.         0.36862746 0.        ]\n",
      "  [0.         0.         0.         0.12156863 0.5921569  0.85882354\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   0.99607843 0.9490196  0.6156863  0.42352942 0.35686275 0.9137255\n",
      "   0.8980392  0.36862746 0.45490196 0.58431375 0.7176471  0.90588236\n",
      "   1.         0.96862745 0.2784314  0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.09019608 0.16862746 0.23529412 0.3019608  0.3647059  0.43137255\n",
      "   0.6784314  0.99607843 0.96862745 1.         1.         1.\n",
      "   1.         1.         1.         0.8980392  0.76862746 0.6039216\n",
      "   0.32156864 0.04313726 0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.39607844 1.         0.6039216  0.13333334 0.16862746\n",
      "   0.6156863  0.9843137  0.12156863 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.5137255  1.         0.4745098  0.\n",
      "   0.12156863 0.9882353  0.4509804  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.00392157 0.6156863  1.         0.67058825\n",
      "   0.06666667 0.90588236 0.5647059  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.00392157 0.42352942 0.96862745\n",
      "   0.88235295 0.94509804 0.6        0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.1764706\n",
      "   0.83137256 1.         0.5058824  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.03529412 0.2901961  0.00784314 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
